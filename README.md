# Mastodon Processor Agent - MLOPS Project (WIP)

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.12+](https://img.shields.io/badge/Python-3.12%2B-yellow.svg)](https://www.python.org/)

This repository is a simple MLOPS project that demonstrates the integration of a Mastodon processor agent with Kafka for data streaming and processing.

<div align="center">
  <img src="./documentation/resources/dashboard-gui-00.png" width="800">
</div>

## Project Overview

The point is to learn building scalable architectures using:
- âœ… Docker (with proper isolation and shared resources)
- âœ… Docker Compose (for local development)
    - âœ… Mounted secrets (not the most secure)
    - advantage: easy to setup + mounted volumes for code changes
- âœ… K8s/Helm (for production-like deployments)
    - âŒ TODO: Vault or K8s secrets
    - advantage: scalable + closer to production environments
- âŒ TODO: Terraform (local AWS simulation with LocalStack)
- âœ… LLM server for LangChain/LangGraph agents
    - âœ… Ollama (qwen3:0.6b)
    - âŒ TODO: vLLM
- âœ… Kafka for data streaming
    - âŒ TODO: Protobuf serialization
- Dashboard
    - âœ… Controller + Visualizer for the Mastodon agent
    - âœ… Django Mono (backend + frontend)
    - Next.js + Tailwind (frontend) âŒ TODO: migrate to this

The word agent here refers to a stateless (no internal memory) automated event-based system. Perhaps, a more appropriate name would be "Mastodon Listener".

The Mastodon agent listens to new posts on a Mastodon instance, processes them using a language model, and streams the results to Kafka for further analysis or storage.

## ğŸš€ Setup Instructions

### Quick Start (Using Docker Compose)

1. Install docker (cli)
2. ğŸ›ï¸ Configure your secrets in `/infra/secrets` file.
3. ğŸ›ï¸ Configure your llm server in `/infra/llm-server.env` file.
4. Start the services:

```bash
cd infra/docker-compose
docker compose up -d --build
```

5. Open the front http://localhost:58005 to access the application.

### Quick Start (Using Kubernetes)

1. Install docker (cli), kind, kubectl, make
2. ğŸ›ï¸ Configure the secrets (not done yet)
3. ğŸ›ï¸ Configure your llm server in `/infra/llm-server.env` file.
4. Create the kind cluster, build and load images, deploy the application:

```bash
cd infra/k8s
make create-cluster

make build # build the images (if not done yet)
make kind-load # load the images into the kind cluster

make deploy # deploy the application
make port-forward # to open localhost:58005
```

5. Open the front http://localhost:58005 to access the application.